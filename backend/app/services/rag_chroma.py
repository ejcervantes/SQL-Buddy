import chromadb
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma

from app.config import settings

class RAGServiceChroma:
    """
    Servicio para gestionar la base de datos vectorial con ChromaDB (persistente).
    """
    def __init__(self):
        """
        Inicializa el servicio RAG, el modelo de embeddings y el cliente de ChromaDB.
        """
        if not settings.validate():
            raise ValueError("Configuraci√≥n inv√°lida. Revisa las variables de entorno.")

        # 1. Inicializar el modelo de embeddings de OpenAI
        self.embeddings = OpenAIEmbeddings(
            openai_api_key=settings.OPENAI_API_KEY
        )

        # 2. Inicializar el cliente de ChromaDB para almacenamiento persistente
        # El directorio se toma de la configuraci√≥n
        self.vector_store = Chroma(
            persist_directory=settings.CHROMA_PERSIST_DIRECTORY,
            embedding_function=self.embeddings
        )

        # 3. Inicializar el divisor de texto
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=100,
            length_function=len
        )
        print(f"‚úÖ Servicio RAG con ChromaDB inicializado. Directorio: {settings.CHROMA_PERSIST_DIRECTORY}")

    def add_table_metadata(self, table_name: str, schema_info: str, description: str) -> bool:
        """
        A√±ade metadatos de una tabla a la base vectorial persistente.
        """
        try:
            # Combinar la informaci√≥n en un solo documento de texto
            content = f"Tabla: {table_name}\nEsquema: {schema_info}\nDescripci√≥n: {description}"
            
            # Dividir el texto en fragmentos (chunks)
            documents = self.text_splitter.create_documents(
                [content], 
                metadatas=[{"table_name": table_name, "source": "metadata"}]
            )
            
            # A√±adir los documentos al vector store (se guardar√°n en disco)
            self.vector_store.add_documents(documents)
            
            print(f"‚úÖ Metadatos de tabla '{table_name}' a√±adidos a ChromaDB.")
            return True
        except Exception as e:
            print(f"‚ùå Error al a√±adir metadatos para '{table_name}': {e}")
            return False

    def search_relevant_tables(self, query: str, top_k: int = 5) -> list:
        """
        Busca en la base vectorial los metadatos de tablas m√°s relevantes para una consulta.
        """
        try:
            results = self.vector_store.similarity_search_with_score(query, k=top_k)
            
            # Formatear los resultados para que sean m√°s f√°ciles de usar
            formatted_results = []
            for doc, score in results:
                formatted_results.append({
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "score": float(score)
                })
            return formatted_results
        except Exception as e:
            print(f"‚ùå Error durante la b√∫squeda de similitud: {e}")
            return []

    def get_available_tables(self) -> list[str]:
        """
        Obtiene una lista de nombres de tablas √∫nicas que han sido a√±adidas.
        """
        try:
            # Obtener todos los documentos almacenados
            # El m√©todo .get() sin filtros devuelve todo
            all_docs = self.vector_store.get()
            
            if not all_docs or not all_docs.get("metadatas"):
                return []

            # Extraer los nombres de las tablas de los metadatos y eliminar duplicados
            table_names = {
                meta['table_name'] 
                for meta in all_docs["metadatas"] 
                if 'table_name' in meta
            }
            return sorted(list(table_names))
        except Exception as e:
            print(f"‚ùå Error obteniendo la lista de tablas: {e}")
            return []

    def query_openai(self, text: str) -> str:
        """M√©todo de prueba para verificar la conexi√≥n con OpenAI."""
        # Este es un m√©todo simple para el health check, no usa el LLM directamente
        # pero confirma que la configuraci√≥n de embeddings (que usa la API key) es v√°lida.
        try:
            self.embeddings.embed_query(text)
            return "OK"
        except Exception as e:
            print(f"Error en query_openai (health check): {e}")
            return f"Error: {e}"
        
    def get_context_for_sql_generation(self, query: str, top_k: int = 3) -> str:
        """
        Busca tablas relevantes y formatea sus metadatos como contexto para el LLM.
        """
        print(f"üîé Buscando contexto para la pregunta: '{query}'")
        relevant_tables = self.search_relevant_tables(query, top_k=top_k)
        
        if not relevant_tables:
            print("‚ö†Ô∏è No se encontraron tablas relevantes para la pregunta.")
            return "No se encontraron metadatos de tablas relevantes."

        # Formatear el contexto para que sea claro para el LLM
        context = "Aqu√≠ est√°n los esquemas de las tablas relevantes para la pregunta:\n\n"
        for table in relevant_tables:
            # table['content'] contiene el texto formateado: "Tabla: ... Esquema: ... Descripci√≥n: ..."
            context += f"---\n{table['content']}\n---\n"
        
        return context